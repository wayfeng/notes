#+TITLE:     Hosting Codellama Locally
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="css/article.css" />
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="css/toc.css" />
#+INDEX:     LLM

* Convert Pytorch checkpoints to GGUF format

The source code of =llama.cpp= is available on GitHub.
#+begin_src bash
  git clone https://github.com/ggerganov/llama.cpp.git
#+end_src

It's better to create a virtual environment to install python dependencies.
#+begin_src bash
  cd llama.cpp
  virtualenv .env
  source .env/bin/activate
  pip install -r ./requirements.txt
#+end_src

To use quantization tool, we need to build =llama.cpp=.
#+begin_src bash
  mkdir build
  cd build
  cmake -DLLAMA_CUBLAS=ON .. # if you have Nvidia GPU and CUDA
  make -j$(nproc)
  cd ..
#+end_src

Convert =codellama= model to GGUF format, and quantize the converted model to INT4.
In this example, I downloaded =codellama-7b= checkpoints in =./models/CodeLlama-7b=.

#+begin_src bash
  python convert.py ./models/CodeLlama-7b --outtype f16
  ./build/bin/quantize ./models/CodeLlama-7b/ggml-model-f16.gguf 2
#+end_src

* Install and setup [[https://ollama.ai][ollama]]

#+begin_src bash
  mkdir -p $HOME/.local/bin
  curl -L https://ollama.ai/download/ollama-linux-amd64 -o $HOME/.local/bin/ollama
  export PATH=$HOME/.local/bin:$PATH
#+end_src

Create Modelfile for ollama.
#+begin_src bash
  cat Modelfile <<EOF > Modelfile
  FROM ./models/CodeLlama-7b/ggml-model-Q4_0.gguf
  EOF
#+end_src

Add model to ollama
#+begin_src bash
  ollama create codellama:7b -f Modelfile
#+end_src

Start ollama server
#+begin_src bash
  export OLLAMA_HOST=0.0.0.0 # in case of remote access of ollama server
  ollama serve
#+end_src

Check ollama installed models
#+begin_src bash
  ollama list
#+end_src

* Configure [[https://continue.dev][Continue]] extension

Add ollama server to Continue configure file =config.json=
#+begin_src json
{
  "models": [
    ...,
    {
      "title": "CodeLlama-7b",
      "model": "codellama:7b",
      "completionOptions": {},
      "apiBase": "http://localhost:11434",
      "provider": "ollama"
    },
    ...
  ]
#+end_src

In case that =ollama= is hosted on a remote server, we can use SSH port forwarding
to let =continue= access the remote service.

#+begin_src bash
  ssh -L 11434:localhost:11434 user@server
#+end_src
