#+TITLE:     Large Language Models and Cross Modal Models
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="css/article.css" />
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="css/toc.css" />
#+HTML_HEAD: <script src="js/mermaid.min.js" type="text/javascript"></script>
#+HTML_HEAD_EXTRA: <script src="js/org-info.js" type="text/javascript"></script>
#+OPTIONS:   tex:t
#+INDEX: deeplearning!llm
#+INDEX: deeplearning!attention
#+INDEX: deeplearning!transformer
#+INDEX: deeplearning!embedding

* Embedding

* Attention

** Transformer

** Visual Transformer

** KV Caching

** [[https://arxiv.org/abs/1911.02150][Multi-Query Attention]]

** [[https://arxiv.org/abs/2305.13245v2][Grouped-Query Attention]]

** [[https://arxiv.org/abs/2309.06180][Paged Attention]]

* Quantization

* RAG

* References

- [[https://simonwillison.net/2023/Oct/23/embeddings/][What are embeddings]]
