#+TITLE:     Time Series Analysis
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="css/article.css" />
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="css/toc.css" />
#+OPTIONS:   tex:t

* Basics

** Time Series

   Values measured repeatedly over time become time series.

** Components of Time Series
   - Trend
   - Seasonal variation
   - Cyclic variation
   - Irregular variation

** Additive Model VS Multiplicative Model
   Multiplicative: Y = T \times S \times C \times I

   Additive: Y = T + S + C + I

** Autocorrelation

   Values of a time series may be correlated with previous values in the same series.

** Coefficient

   Given a pair of random variables $X, Y$,

   \begin{equation}
   \rho_{X,Y} = \frac{cov(X, Y)}{\sigma_X\sigma_Y}
   \end{equation}

   Since,

   \begin{equation}
   cov(X,Y) = \mathbb{E}[(X - \mu_X)(Y - \mu_Y)],
   \end{equation}

   $\rho$ can be written as

   \begin{equation}
   \rho_{X,Y} = \frac{\mathbb{E}[(X-\mu_X)(Y-\mu_Y)]}{\sigma_X\sigma_Y}
   \end{equation}

   where:

   - $\sigma_X$ and $\sigma_Y$ are standard deviation of $X$ and $Y$
   - $\mu_X$ and $\mu_Y$ are mean of $X$ and $Y$
   - $\mathbb{E}$ is the expectation.

** ACF (Auto Correlation Function) & PACF (Partial ACF)

   The partial autocorrelation function gives the partial correlation of a
   stationary time series with its own lagged values.

   Given a time series $y_t$, the partial correlation of lag $k$, denoted
   $\alpha_k$, is autocorrelation between $y_t$ and $y_{t-k}$.

   \begin{equation}
   y_{t} = \alpha_{0} + \alpha_{1} y_{t-1} + \alpha_{2} y_{t-2} + \dots + \alpha_{k} y_{t-k} + \epsilon_{t}
   \end{equation}

** AutoRegression
   \begin{align}
   AR(1) &\rightarrow y_t = \alpha_1 y_{t-1} + \epsilon_t \\
   AR(2) &\rightarrow y_t = \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \epsilon_t \\
   AR(p) &\rightarrow y_t = \alpha_1 y_{t-1} + \alpha_2 y_{t-2} + \dots + \alpha_p y_{t-p} + \epsilon_t \\
   \end{align}


** Stationary

*** requirements
   - mean $\mu$ is constant
   - standard deviation $\sigma$ is constant
   - no *seasonality* component

*** tests
   - visual
   - local test V.S. global test
   - Augmented Dickey-Fuller Test

** Integrated (Differentiate)
   Integrating can be used to make a time-series stationary.

   For non-stationary time-series
   \begin{align}
   y_t &= \beta_0 + \beta_{t} t + \epsilon_t \\

   z_t &= y_t - y_{t-k} \\
       &= (\beta_{t} - \beta_{t-k}) t + (\epsilon_t - \epsilon_{t-k})
   \end{align}

** Unit Root
   If a time series has unit root, it is non-stationary but does not always have a trend.

   For AR(1) model y_t = \alpha y_{t-1} + \epsilon_t, where \epsilon \sim \Nu(0, \sigma^2),
   \begin{align}
   \mathbb{E}(y_t) &= \alpha\mathbb{E}(y_{t-1}) + \mathbb{E}(\epsilon_t) = \alpha^t y_0 \\
   var(y_t) &= \alpha^2 var(y_{t-1}) + var(\epsilon_t) = \alpha^2 var(y_{t-1}) + \sigma^2 \\
            &= \sigma^2\sum_{k=0}^{t-1}{\alpha^{2k}} \\
   \end{align}

   - If $\alpha > 1$, both mean and variance are infinite overtime.
   - If $\alpha = 1$, the process has a unit root. $\mathbb{E}(y_t)$ is constant but $\lim_{t \to \infty}var(y_t) \to \infty$.
   - $y_t$ is stationary only when $|\alpha| < 1$.


** Moving Average
   For time series $y_t$ with error \epsilon \sim \Nu(\mu_\epsilon, \sigma_\epsilon^2),

   \begin{align}
   MA(1) &\rightarrow \hat{y}_t = \mu + \beta_1 \epsilon_{t-1} \\
   MA(2) &\rightarrow \hat{y}_t = \mu + \beta_1 \epsilon_{t-1} + \beta_2 \epsilon_{t-2} \\
   MA(q) &\rightarrow \hat{y}_t = \mu + \beta_1 \epsilon_{t-1} + \beta_2 \epsilon_{t-2} + \cdots + \beta_q \epsilon_{t-q} \\
   \end{align}

** Exogenous Variables
   A parallel time series that not modeled directly but is used as a weighted input to the model.

   e.g. Consider daily based number of customers of a restaurant. Whether the day is weekend or holiday will affect the number.

** AIC (Akaike Information Criterion)
   \begin{equation}
   AIC = 2k - 2\ln(\hat{L})
   \end{equation}

** BIC (Bayesian Information Criterion)
   \begin{equation}
   BIC = 2 \ln{N} k - 2 \ln{L}
   \end{equation}
** HQIC
** Invertible

* Models
** ARMA(p, q)
   \begin{equation}
   y_t = \phi_1 y_{t-1} + \phi_2 y_{t-2} + \ldots + \phi_p y_{t-p} + \theta_1\epsilon_{t-1}
         + \theta_2 \epsilon_{t-2} + \ldots + \theta_q \epsilon_{t-q} + \epsilon_t
   \end{equation}
** ARIMA(p, d, q)
   \begin{equation}
   \left(1-\sum_{i=1}^p\phi_i L^i\right)(1-L)^d X_t = \left(1+\sum^q_{i=1}\theta_iL^i\right)\epsilon_t
   \end{equation}

** SARIMAX

** VAR
   
   2-dimensional VAR(1) model:
   \begin{align}
   y_{1,t} &= \phi_{1,1}y_{1,t-1} + \phi_{1,2}y_{2,t-1} + \epsilon_{1,t} \\
   y_{2,t} &= \phi_{2,1}y_{1,t-1} + \phi_{2,2}y_{2,t-1} + \epsilon_{2,t} \\
   \end{align}

** VARMA

   2-dimensional VARMA(1,1) model:
   \begin{align}
   y_{1,t} &= \phi_{1,1}y_{1,t-1} + \phi_{1,2}y_{2,t-1} + \theta_{1,1}\epsilon_{1,t-1} + \theta_{1,2}\epsilon_{2,t-1} + \epsilon_{1,t} \\
   y_{2,t} &= \phi_{2,1}y_{1,t-1} + \phi_{2,2}y_{2,t-1} + \theta_{2,1}\epsilon_{1,t-1} + \theta_{2,2}\epsilon_{2,t-1} + \epsilon_{2,t} \\
   \end{align}

* Tools
** Python
   - Numpy
   - Pandas
   - Statsmodels.tsa
   - pdmarima
   - Scipy

* Projects
