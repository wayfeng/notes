#+TITLE: Bootstrapping k8s cluster on KVM/libvirt
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="css/article.css" />
#+HTML_HEAD: <link rel="stylesheet" type="text/css" href="css/toc.css" />
#+INDEX: k8s kvm libvirt

* KVM & qemu & libvirt

** KVM
** qemu
** libvirt

** Setup host system
#+BEGIN_SRC bash
sudo apt install qemu-kvm qemu-system-x86
sudo apt install libvirt-clients libvirt-daemon-system virtinst
sudo usermod -aG kvm $USER
sudo usermod -aG libvirt $USER
kvm-ok
  INFO: /dev/kvm exists
  KVM acceleration can be used
#+END_SRC

* Using Bridged Network

** Creating bridge

#+BEGIN_SRC bash
sudo apt install bridge-utils
sudo brctl addbr br0
sudo brctl addif br0 eno1
#+END_SRC

** Bringing up bridge

#+BEGIN_SRC bash
cat <<EOF | sudo tee /etc/network/interfaces.d/br0
iface br0 inet dhcp
    bridge_ports eno1
EOF
#+END_SRC

** Bringing up bridge with netplan

#+BEGIN_SRC bash
echo 'network:
    ethernets:
      eno1:
        dhcp4: no
    bridges:
      br0:
        dhcp4: yes
        interfaces:
          - eno1
    version: 2' | sudo tee /etc/netplan/00-installer-config.yaml
sudo netplan apply
#+END_SRC

** Applying bridged network to libvirt (optional)

#+BEGIN_SRC bash
echo '<network>
    <name>bridged-network</name>
    <forward mode="bridge" />
    <bridge name="br0" />
  </network>' > /tmp/bridged-network.xml
virsh net-define /tmp/bridged-network.xml
virsh net-start bridged-network
virsh net-autostart bridged-network
#+END_SRC

* Bootstrap k8s cluster

** producing ignite file
   Example yaml file with proxy and k8s repo setting. Check out [[https://docs.fedoraproject.org/en-US/fedora-coreos/producing-ign/][ref]] for details.
#+BEGIN_SRC yaml
  variant: fcos
  version: 1.4.0
  passwd:
    users:
      - name: core
        groups:
          - docker
          - wheel
          - sudo
        password_hash: ...
        ssh_authorized_keys:
          - ssh-rsa AAAAB...
  storage:
    directories:
      - path: /etc/systemd/system/rpm-ostreed.service.d
        overwrite: true
      - path: /etc/systemd/system/docker.service.d
        overwrite: true
    files:
      - path: /etc/sysctl.d/20-silence-audit.conf
        contents:
          inline: |
            kernel.printk=4
      - path: /etc/systemd/system/rpm-ostreed.service.d/http-proxy.conf
        contents:
          inline: |
            [Service]
            Environment="http_proxy=<proxy>"
      - path: /etc/systemd/system/docker.service.d/http-proxy.conf
        contents:
          inline: |
            [Service]
            Environment="http_proxy=<proxy>"
      - path: /etc/systemd/system/docker.service.d/https-proxy.conf
        contents:
          inline: |
            [Service]
            Environment="https_proxy=<proxy>"
      - path: /etc/yum.repos.d/kubernetes.repo
        contents:
          inline: |
            [kubernetes]
            name=Kubernetes
            baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
            enabled=1
            gpgcheck=1
            gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
      - path: /etc/hostname
        mode: 420
        contents:
          inline: fcos
#+END_SRC

To produce ignite file, I prefer to run butane tool in a container.
#+BEGIN_SRC bash
docker run -it --rm --volume $(pwd):/data --workdir /data \
  quay.io/coreos/butane:release --pretty --strict conf.yaml -o conf.ign
#+END_SRC

** Download Fedora CoreOS image

** Start a VM as k8s master
   The guest system has 2 cores, 2048MB RAM and 10GB disk. For more details, see [[https://docs.fedoraproject.org/en-US/fedora-coreos/getting-started/][ref]].

   To make sure each VM has it's own unique hostname, a ignite file (with
   different hostname) is needed. There might be a better way to achieve this.
#+BEGIN_SRC bash
virt-install --connect="qemu:///system" --name="fcos-master-01" --vcpus=2 \
             --memory=2048 --disk="size=10,backing_store=${IMG}" \
             --os-variant="fedora-unknown" --import --graphcs=none \
             --noautoconsole --network network=bridged-network \
             --qemu-commandline="-fw_cfg name=opt/com.coreos/config,file=${IGN}"
#+END_SRC

   You can use =virsh= tool to check the VM just created.
#+BEGIN_SRC bash
virsh list
 Id   Name             State
--------------------------------
  1   fcos-master-01   running
#+END_SRC

Since option =noautoconsole= was added to force the VM created in background,
You will need to attach to it's console manually.
#+BEGIN_SRC bash
virsh console fcos-master-01
#+END_SRC

** Setup k8s repo (can be done in ignite file)
#+BEGIN_SRC bash
[core@fcos-master-01 ~]$ echo '[kubernetes]
  name=Kubernetes
  baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
  enabled=1
  gpgcheck=1
  repo_gpgcheck=1
  gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg' \
 | sudo tee /etc/yum.repos.d/kubernetes.repo
#+END_SRC

** Install k8s tools.
#+BEGIN_SRC bash
[core@fcos-master-01 ~]$ sudo rpm-ostree install kubadmin kubelet kubectl
[core@fcos-master-01 ~]$ sudo systemctl reboot
#+END_SRC

  Enable needed services.
#+BEGIN_SRC bash
[core@fcos-master-01 ~]$ sudo systemctl enable docker
[core@fcos-master-01 ~]$ sudo systemctl enable kubelet
#+END_SRC

  Disable SELinux if necessary.
#+BEGIN_SRC bash
[core@fcos-master-01 ~]$ sudo setenforce 0
[core@fcos-master-01 ~]$ sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config
#+END_SRC

** Configure k8s master
   The default =flex-volum-plugin-dir= on CoreOS is read-only, so we need to use a different folder, otherwise the =kube-controller-manager= pod won't be running.
#+BEGIN_SRC bash
[core@fcos-master-01 ~]$ cat <<EOF > kubeadm-custom.yml
apiVersion: kubeadm.k8s.io/v1beta2
kind: ClusterConfiguration
kubernetesVersion: v1.17.0
controllerManager:
  extraArgs:
    flex-volume-plugin-dir: "/etc/kubernetes/kubelet-plugins/volume/exec"
networking:
  podSubnet: 10.244.0.0/16
EOF
[core@fcos-master-01 ~]$ sudo kubeadm init --config kubeadm-custom.yml
#+END_SRC

** Install Pod network add-on

#+BEGIN_SRC bash
[core@fcos-master-01 ~]$ curl -OL https://docs.projectcalico.org/manifests/calico.yaml
[core@fcos-master-01 ~]$ sed -i s'!/usr/libexec/kubernetes!/etc/kubernetes!' calico.yaml
# login to docker to pull calico images
[core@fcos-master-01 ~]$ sudo docker login
[core@fcos-master-01 ~]$ kubectl apply -f calico.yaml

# check if pods are running
[core@fcos-master-01 ~]$ kubectl get pods --all-namespaces

# check if nodes are ready
[core@fcos-master-01 ~]$ kubectl get nodes
#+END_SRC

** Join worker notes
#+BEGIN_SRC bash
[core@fcos-worker-01 ~]$ sudo kubeadm join <master>:6443 --token <token> --discovery-token-ca-cert-hash sha256:<hash>
# check nodes again
[core@fcos-master-01 ~]$ kubectl get node
#+END_SRC

** Create pods

Docker Hub has limitation for unregistered user, make sure that your k8s
cluster knows how to login to Docker Hub with *secret*.

First get login info.
#+BEGIN_SRC bash
[core@fcos ~]$ docker login
[core@fcos ~]$ cat ~/.docker/config.json
#+END_SRC

It looks like this.
#+BEGIN_SRC json
{
    "auths": {
        "https://index.docker.io/v1/": {
            "auth": "c3R...zE2"
        }
    }
}
#+END_SRC

Then create *secret* with =kubectl=.
#+BEGIN_SRC bash
[core@fcos ~]$ kubectl create secret generic regcred \
    --from-file=.dockerconfigjson=/home/core/.docker/config.json \
    --type=kubernetes.io/dockerconfigjson
#+END_SRC

Check the *secret* you just created.
#+BEGIN_SRC bash
[core@fcos ~]$ kubectl get secret regcred --output=yaml
#+END_SRC

It looks like this.
#+BEGIN_SRC yaml
apiVersion: v1
kind: Secret
metadata:
  ...
  name: regcred
  ...
data:
  .dockerconfigjson: ...
type: kubernetes.io/dockerconfigjson
#+END_SRC

To check =.dockerconfigjson= contents.
#+BEGIN_SRC bash
[core@fcos ~]$ kubectl get secret regcred --output="jsonpath={.data.\.dockerconfigjson}" | base64 --decode
#+END_SRC

Now create yaml file that uses the secret to create deployment.
#+BEGIN_SRC bash
[core@fcos ~]$ cat nginx.yml
#+END_SRC

#+BEGIN_SRC yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:stable
        ports:
        - containerPort: 80
      imagePullSecrets:
      - name: regcred
#+END_SRC

Then create pods with it.
#+BEGIN_SRC bash
[core@fcos ~]$ kubectl apply -f nginx.yml
#+END_SRC

#+BEGIN_SRC bash
[core@fcos ~]$ kubectl expose deployment nginx-deployment --port=80 --type=NodePort
#+END_SRC

* FAQs

** No permission to access ignite file
   This is an issue of AppArmor ([[https://unix.stackexchange.com/questions/578086/virt-install-error-cant-load-ignition-file][ref]]). The fix is add path of ignite files to
   apparmor configure file.
#+BEGIN_SRC sh
echo '#include <tunables/global>
profile LIBVIRT_TEMPLATE flags=(attach_disconnected) {
  #include <abstractions/libvirt-qemu>
  /var/lib/libvirt/images/**.ign rk,
}' | sudo tee /etc/apparmor.d/libvirt/TEMPLATE.qemu
#+END_SRC

** Using rpm-ostree behind proxy

#+BEGIN_SRC bash
[core@fcos ~]$ sudo mkdir -p /etc/systemd/system/rpm-ostreed.service.d
[core@fcos ~]$ echo '[Service]
Environment="http_proxy=http://<my-proxy>"' | \
sudo tee /etc/systemd/system/rpm-ostreed.service.d/http-proxy.conf
[core@fcos ~]$ systemctl daemon-reload
[core@fcos ~]$ systemctl restart rpm-ostreed.service
#+END_SRC

** Using docker behind proxy

#+BEGIN_SRC bash
[core@fcos ~]$ echo '[Service]
Environment="HTTP_PROXY=<proxy>"' | \
sudo tee /etc/systemd/system/docker.service.d/http-proxy.conf
[core@fcos ~]$ echo '[Service]
Environment="HTTPS_PROXY=<proxy>"' | \
sudo tee /etc/systemd/system/docker.service.d/https-proxy.conf
#+END_SRC
